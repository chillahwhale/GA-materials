{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://imgur.com/1ZcRyrc.png\" style=\"float: left; margin: 20px; height: 55px\">\n",
    "\n",
    "## Classification Techniques\n",
    "\n",
    "_Your one-stop-shop for keeping classification techniques and code handy_\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will split up the work for this notebook:\n",
    "- Person 1: Logistic Regression, Regularized Logistic Regression\n",
    "- Person 2: kNN\n",
    "- Person 3: Decision Tree and Random Forest Classifier\n",
    "- Person 4: Naive Bayes Classifier\n",
    "- Person 5: Support Vector Machines\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classification models covered in this notebook:\n",
    "- [Logistic Regression](#Logistic-Regression)\n",
    "    - [Regularized Logistic Regression](#Regularized-Logistic-Regression)\n",
    "- [K-Nearest Neighbor](#kNN)\n",
    "- [Decision Tree](#Decision-Tree)\n",
    "- [Random Forest Classifier](#Random-Forest-Classifier)\n",
    "- [Naive Bayes Classifier](#Naive-Bayes-Classifier)\n",
    "- [Support Vector Machines](#Support-Vector-Machines)\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/noellebrown/opt/anaconda3/lib/python3.7/importlib/_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192 from C header, got 216 from PyObject\n",
      "  return f(*args, **kwds)\n",
      "/Users/noellebrown/opt/anaconda3/lib/python3.7/site-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
      "  import pandas.util.testing as tm\n",
      "/Users/noellebrown/opt/anaconda3/lib/python3.7/importlib/_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192 from C header, got 216 from PyObject\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "# imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>admit</th>\n",
       "      <th>gre</th>\n",
       "      <th>gpa</th>\n",
       "      <th>prestige</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>380.0</td>\n",
       "      <td>3.61</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>660.0</td>\n",
       "      <td>3.67</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>800.0</td>\n",
       "      <td>4.00</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>640.0</td>\n",
       "      <td>3.19</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>520.0</td>\n",
       "      <td>2.93</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   admit    gre   gpa  prestige\n",
       "0      0  380.0  3.61       3.0\n",
       "1      1  660.0  3.67       3.0\n",
       "2      1  800.0  4.00       1.0\n",
       "3      1  640.0  3.19       4.0\n",
       "4      0  520.0  2.93       4.0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read in the data.\n",
    "admissions = pd.read_csv('./data/admissions.csv')\n",
    "admissions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "admit       0\n",
       "gre         2\n",
       "gpa         2\n",
       "prestige    1\n",
       "dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "admissions.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "admissions.dropna(axis=0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into testing and training sets\n",
    "features = ['gre', 'gpa', 'prestige']\n",
    "X = admissions[features] # feature matrix\n",
    "y = admissions['admit'] # target vector\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Logistic Regression\n",
    "_Noelle_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When should you use the algorithm?\n",
    "> **ANSWER HERE**: When you are trying to predict a binary outcome or are trying to find the probability of being in each class. Should also be used when you need a very interpretable classification model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are some benefits and drawbacks of the model?\n",
    "> **Benefits:**  \n",
    "- Simple\n",
    "- Can perform well even for complex problems\n",
    "- Interpretable\n",
    "- Can see both predicted probability of being in your target class in addition to your prediction\n",
    "\n",
    "> **Drawbacks:**  \n",
    "- Assumption of linearity\n",
    "- Can easily be overfit\n",
    "\n",
    "> [more](https://theprofessionalspoint.blogspot.com/2019/03/advantages-and-disadvantages-of.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Give us an example of how to use it in Python using the provided dataset\n",
    "> **ANSWER BELOW**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Metrics\n",
      "---------------------------\n",
      "Training accuracy: 0.734006734006734\n",
      "Testing accuracy: 0.61\n",
      " \n",
      "Training Recall: 0.24719101123595505\n",
      "Testing Recall: 0.16216216216216217\n",
      " \n",
      "Training Precision: 0.6470588235294118\n",
      "Testing Precision: 0.42857142857142855\n",
      " \n",
      "Training F1: 0.35772357723577236\n",
      "Testing F1: 0.23529411764705885\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\n",
    "\n",
    "log_reg = LogisticRegression(penalty='none', solver='lbfgs') # instantiate & turn off regularization\n",
    "log_reg.fit(X_train, y_train) # fit\n",
    "\n",
    "# simple evaluation function for scoring\n",
    "def classification_evaluate(y_train_true, y_train_pred, y_test_true, y_test_pred):\n",
    "    print('Evaluation Metrics')\n",
    "    print('---------------------------')\n",
    "    print('Training accuracy:', accuracy_score(y_train_true, y_train_pred))\n",
    "    print('Testing accuracy:', accuracy_score(y_test_true, y_test_pred))\n",
    "    print(' ')\n",
    "    print('Training Recall:', recall_score(y_train_true, y_train_pred))\n",
    "    print('Testing Recall:', recall_score(y_test_true, y_test_pred))\n",
    "    print(' ')\n",
    "    print('Training Precision:', precision_score(y_train_true, y_train_pred))\n",
    "    print('Testing Precision:', precision_score(y_test_true, y_test_pred))\n",
    "    print(' ')\n",
    "    print('Training F1:', f1_score(y_train_true, y_train_pred))\n",
    "    print('Testing F1:', f1_score(y_test_true, y_test_pred))\n",
    "\n",
    "# score\n",
    "classification_evaluate(y_train, log_reg.predict(X_train), y_test, log_reg.predict(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Regularized Logistic Regression\n",
    "_Noelle_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When should you use the algorithm?\n",
    "> **ANSWER HERE**: When your logistic regression is overfit and needs to be regularized. Note that logistic regression in sklearn is regularized by default."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are some benefits and drawbacks of the model?\n",
    "> **ANSWER HERE**: Same as above! Another benefit would be that it prevents overfitting. When using the l1 norm as a penalty, you can actually zero out coefficients."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Give us an example of how to use it in Python using the provided dataset\n",
    "> **ANSWER BELOW**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Metrics\n",
      "---------------------------\n",
      "Training accuracy: 0.734006734006734\n",
      "Testing accuracy: 0.61\n",
      " \n",
      "Training Recall: 0.24719101123595505\n",
      "Testing Recall: 0.16216216216216217\n",
      " \n",
      "Training Precision: 0.6470588235294118\n",
      "Testing Precision: 0.42857142857142855\n",
      " \n",
      "Training F1: 0.35772357723577236\n",
      "Testing F1: 0.23529411764705885\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    0.5s finished\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# scale\n",
    "sc = StandardScaler()\n",
    "X_train_sc = sc.fit_transform(X_train)\n",
    "X_test_sc = sc.transform(X_test)\n",
    "\n",
    "# get params to gridsearch over with various regularization types/strengths\n",
    "log_params = {\n",
    "    'penalty': ['l1', 'l2'],\n",
    "    'solver': ['liblinear'],\n",
    "    'C': np.logspace(0, 1, 10)\n",
    "}\n",
    "\n",
    "# instantiate\n",
    "log_gridsearch = GridSearchCV(LogisticRegression(), \n",
    "                              log_params,\n",
    "                              cv=5,\n",
    "                              verbose=1)\n",
    "\n",
    "# fit\n",
    "log_gridsearch.fit(X_train_sc, y_train)\n",
    "\n",
    "# save best model from gridsearch\n",
    "best_model = log_gridsearch.best_estimator_\n",
    "\n",
    "# evaluate model\n",
    "classification_evaluate(y_train, best_model.predict(X_train_sc), y_test, best_model.predict(X_test_sc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# kNN\n",
    "_Colin_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When should you use the algorithm?\n",
    "> **ANSWER HERE**: \n",
    "- Useful for nonlinear data\n",
    "- \"Should I give you a loan? Do people with your characteristics tend to default on loans?\"\n",
    "- Easy to interpret visually, can be plotted\n",
    "- ML map: https://scikit-learn.org/stable/_static/ml_map.png\n",
    "- Source used: https://blog.usejournal.com/a-quick-introduction-to-k-nearest-neighbors-algorithm-62214cea29c7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are some benefits and drawbacks of the model?\n",
    "> **ANSWER HERE**: \n",
    "- Data is stored within algorithm\n",
    "- High computation power required\n",
    "- More user intuition or trials may be required to remove less useful features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Give us an example of how to use it in Python using the provided dataset\n",
    "> **ANSWER BELOW**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/noellebrown/opt/anaconda3/lib/python3.7/importlib/_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192 from C header, got 216 from PyObject\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train score: 0.7643097643097643\n",
      "Test score: 0.7643097643097643\n",
      "Cross Val score: 0.6970607553366175\n"
     ]
    }
   ],
   "source": [
    "# Scale\n",
    "ss = StandardScaler()\n",
    "X_train = ss.fit_transform(X_train)\n",
    "X_test = ss.transform(X_test)\n",
    "# instantiate\n",
    "knn = KNeighborsClassifier()\n",
    "# fit\n",
    "knn.fit(X_train, y_train)\n",
    "# preliminary scores\n",
    "print(f'Train score: {knn.score(X_train, y_train)}')\n",
    "print(f'Test score: {knn.score(X_train, y_train)}')\n",
    "print(f'Cross Val score: {cross_val_score(knn, X_train, y_train, cv=10).mean()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[47, 16],\n",
       "       [24, 13]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate\n",
    "from sklearn.metrics import confusion_matrix\n",
    "y_preds = knn.predict(X_test)\n",
    "confusion_matrix(y_test, y_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Decision Tree\n",
    "_Matt_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What is a Decision Tree?**\n",
    "---\n",
    "- Decision Trees are a type of Supervised Machine Learning (that is you explain what the input is and what the corresponding output is in the training data) where the data is continuously split according to a certain parameter.\n",
    "- The goal is to create a model that predicts the value of a target variable based on several input variables.\n",
    "- The tree can be explained by two entities, namely decision nodes and leaves.\n",
    "    - The leaves are the decisions or the final outcomes. And the decision nodes are where the data is split.\n",
    "- Two main types:\n",
    "    - Classification trees (Yes/No types)\n",
    "    - Regression trees (Continuous data types)\n",
    "- Two main metrics:\n",
    "    - Gini Impurity (aka entropy)\n",
    "        - measures the amount of uncertainty at a node\n",
    "        - 0 means no uncertainty\n",
    "        - 1 - chances of being right\n",
    "    - Information gain\n",
    "        - Effectively the change in entropy\n",
    "        - let’s us find the question that reduces uncertainty the most\n",
    "            - this is how we determine our root node, or node from which we ask our first question\n",
    "        - once IG = 0 we get a leaf\n",
    "    - *Variance reduction**\n",
    "        - Introduced in CART, often employed with regression trees\n",
    "            - CART (Classification and Regression Tree) is one of many decision tree-specific algorithms\n",
    "**...if you want to know more**\n",
    "- Decision Tree models are created using 2 steps: **Induction and Pruning**.\n",
    "    - **Induction** is where we actually build the tree i.e set all of the hierarchical decision boundaries based on our data. Because of the nature of training decision trees they can be prone to major overfitting.\n",
    "    - **Pruning** is the process of removing the unnecessary structure from a decision tree, effectively reducing the complexity to combat overfitting with the added bonus of making it even easier to interpret.\n",
    "- There are several parameters that you can set for your decision tree model in Scikit Learn too. Here are a few of the more interesting ones to play around with to try and get some better results:\n",
    "     - `max_depth`: The max depth of the tree where we will stop splitting the nodes. This is similar to controlling the maximum number of layers in a deep neural network. Lower will make your model faster but not as accurate; higher can give you accuracy but risks overfitting and may be slow.\n",
    "    - `min_samples_split`: The minimum number of samples required to split a node. We discussed this aspect of decision trees above and how setting it to a higher value would help mitigate overfitting.\n",
    "    - `max_features`: The number of features to consider when looking for the best split. Higher means potentially better results with the tradeoff of training taking longer.\n",
    "    - `min_impurity_split`: Threshold for early stopping in tree growth. A node will split if its impurity is above the threshold. This can be used to tradeoff combating overfitting (high value, small tree) vs high accuracy (low value, big tree).\n",
    "    - `presort`: Whether to presort the data to speed up the finding of best splits in fitting. If we sort our data on each feature beforehand, our training algorithm will have a much easier time finding good values to split on.\n",
    "**When should you use the algorithm?**\n",
    "---\n",
    "- Classification with many categories\n",
    "- We have large data set\n",
    "- Data is both numerical and categorical\n",
    "- Yes/No problems (boolean logic)\n",
    "**What are some benefits and drawbacks of the model?**\n",
    "---\n",
    "**Benefits:**\n",
    "- Simple to understand and to interpret. Trees can be visualized.\n",
    "- Requires little data preparation beyond treating null values\n",
    "- Able to handle both numerical and categorical data.\n",
    "- Uses a white box model. If a given situation is observable in a model, the explanation for the condition is easily explained by boolean logic.\n",
    "- Possible to validate a model using statistical tests.\n",
    "- Performs well even if its assumptions are somewhat violated by the true model from which the data were generated.\n",
    "**Drawbacks:**\n",
    "- High potential for overfitting. Decision-tree learners can create over-complex trees that do not generalize the data well.\n",
    "- Decision trees can be unstable because small variations in the data might result in a completely different tree being generated.\n",
    "- There are concepts that are hard to learn because decision trees do not express them easily, such as XOR, parity or multiplexer problems.\n",
    "- Decision tree learners create biased trees if some classes dominate. It is therefore recommended to balance the dataset prior to fitting with the decision tree.\n",
    "***sources:***\n",
    "- https://www.kdnuggets.com/2018/12/guide-decision-trees-machine-learning-data-science.html\n",
    "- https://en.wikipedia.org/wiki/Decision_tree_learning\n",
    "- https://www.kdnuggets.com/2018/12/guide-decision-trees-machine-learning-data-science.html\n",
    "- https://medium.com/datadriveninvestor/decision-trees-lesson-101-f00dad6cba21\n",
    "- https://github.com/random-forests/tutorials/blob/master/decision_tree.ipynb\n",
    "- https://www.youtube.com/watch?v=LDRbO9a6XPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "code is taken directly from this article: https://towardsdatascience.com/decision-tree-in-python-b433ae57fb93"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Random Forest Classifier\n",
    "_Matt_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What is a Random Forest?**\n",
    "---\n",
    "- Like Decision Trees, Random Forests are a method for classification and regression\n",
    "- RFs construct a multitude of decisoin trees when training the data then output the class that is the mode (i.e. occurs most often) for classification or the mean prediction for regression of the individual trees.\n",
    "- Help to correct decision trees tendency to overfit the trainin set\n",
    "- “*A large number of relatively uncorrelated models (trees) operating as a committee will outperform any of the individual constituent models.*”\n",
    "    - The reason for this wonderful effect is that the trees protect each other from their individual errors\n",
    "- So in our random forest, we end up with trees that are not only trained on different sets of data (thanks to bagging) but also use different features to make decisions.\n",
    "- **IN SHORT: The random forest is a classification algorithm consisting of many decisions trees. It uses bagging and feature randomness when building each individual tree to try to create an uncorrelated forest of trees whose prediction by committee is more accurate than that of any individual tree.**\n",
    "**When should you use the algorithm?**\n",
    "---\n",
    "- If Decision Tree is consistently overfitting\n",
    "- Two the prerequisites for random forest to perform well are:\n",
    "    - There needs to be some actual signal in our features so that models built using those features do better than random guessing.\n",
    "    - The predictions (and therefore the errors) made by the individual trees need to have low correlations with each other.\n",
    "- When you have time to train the data\n",
    "**What are some of the benefits and drawbacks?**\n",
    "---\n",
    "**Benefits**\n",
    "1. Random Forest is based on the bagging algorithm and uses Ensemble Learning technique. It creates as many trees on the subset of the data and combines the output of all the trees. In this way it **reduces overfitting problem in decision trees and also reduces the variance and therefore improves the accuracy.**\n",
    "2. Random Forest can be used to **solve both classification as well as regression problems**.\n",
    "3. Random Forest **works well with both categorical and continuous variables**.\n",
    "4. Random Forest can **automatically handle missing values.**\n",
    "5. **No feature scaling required**: No feature scaling (standardization and normalization) required in case of Random Forest as it uses rule based approach instead of distance calculation.\n",
    "6. **Handles non-linear parameters efficiently**: Non linear parameters don’t affect the performance of a Random Forest unlike curve based algorithms. So, if there is high non-linearity between the independent variables, Random Forest may outperform as compared to other curve based algorithms.\n",
    "7. Random Forest is **usually robust to outliers** and can handle them automatically.\n",
    "    - Random Forest algorithm is **very stable**. Even if a new data point is introduced in the dataset, the overall algorithm is not affected much since the new data may impact one tree, but it is very hard for it to impact all the trees.\n",
    "    - Random Forest is comparatively less impacted by noise.\n",
    "**Drawbacks**\n",
    "1. **Complexity**: Random Forest creates a lot of trees (unlike only one tree in case of decision tree) and combines their outputs. By default, it creates 100 trees in Python sklearn library. To do so, this algorithm requires much more computational power and resources. On the other hand decision tree is simple and does not require so much computational resources.\n",
    "2. **Longer Training Period**: Random Forest require much more time to train as compared to decision trees as it generates a lot of trees (instead of one tree in case of decision tree) and makes decision on the majority of votes.\n",
    "***sources:***\n",
    "- https://www.stat.berkeley.edu/~breiman/randomforest2001.pdf\n",
    "- https://en.wikipedia.org/wiki/Random_forest\n",
    "- https://towardsdatascience.com/understanding-random-forest-58381e0602d2\n",
    "- http://theprofessionalspoint.blogspot.com/2019/02/advantages-and-disadvantages-of-random.html\n",
    "- https://github.com/WillKoehrsen/Machine-Learning-Projects/blob/master/Random%20Forest%20Tutorial.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Naive Bayes Classifier\n",
    "_Justin_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.machinelearningplus.com/predictive-modeling/how-naive-bayes-algorithm-works-with-example-and-full-code/\n",
    "https://becominghuman.ai/naive-bayes-theorem-d8854a41ea08\n",
    "https://monkeylearn.com/blog/practical-explanation-naive-bayes-classifier/ -> Good explanation on how it works\n",
    "https://www.geeksforgeeks.org/naive-bayes-classifiers/\n",
    "https://towardsdatascience.com/naive-bayes-classifier-81d512f50a7c\n",
    "https://www.analyticsvidhya.com/blog/2017/09/naive-bayes-explained/\n",
    "\n",
    "When should you use the algorithm?\n",
    "> **ANSWER HERE**: \n",
    "> The name naive is used because it assumes the features that go into the model is independent of each other. That is changing the value of one feature, does not directly influence or change the value of any of the other features used in the algorithm.\n",
    "> The Bayes Rule: The Bayes Rule is a way of going from P(X|Y), known from the training dataset, to find P(Y|X).   \n",
    "$P(A|B)= \\frac{P(B|A)P(A)}{P(B)}$\n",
    "> $P(Yes|conds)=\\frac{P(X_1|Yes)*P(X_2|Yes)*...*P(X_n|Yes)}{P(conds)}$\n",
    "> $P(No|conds)=\\frac{P(X_1|No)*P(X_2|No)*...*P(X_n|No)}{P(conds)}$\n",
    "> Since both have the same denominator, it can be ignored.  Just care about each numerator.  The greater one is selected\n",
    "\n",
    "What are some benefits and drawbacks of the model?\n",
    "> **Benefits**:\n",
    "- Quick and efficient\n",
    "- Outperforms other models when indepences holds.\n",
    "\n",
    ">**Drawbacks**:\n",
    "- Features ***NEED*** to be independent.  They aren't in the real world most of the time\n",
    "- Data will need to be smoothed to ensure there is no condition of a feature in the test set that isn't in the training set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Give us an example of how to use it in Python using the provided dataset\n",
    "> **ANSWER BELOW**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train score: 0.72391\n",
      "Test score: 0.63\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "gnb = GaussianNB()\n",
    "gnb.fit(X_train, y_train)\n",
    "y_pred = gnb.predict(X_test)\n",
    "print(f'Train score: {gnb.score(X_train, y_train):.5}')\n",
    "print(f'Test score: {gnb.score(X_test, y_test):.5}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Support Vector Machines\n",
    "_Nate_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **What is SVM**:\n",
    "- The objective of the support vector machine algorithm is to find a hyperplane in an N-dimensional (N = the number of features) that distinctly classifies the data points.\n",
    "- We are looking to maximize the margin between the data points and the hyperplane.\n",
    "- Hyperplanes are decision boundaries that help classify the data points.\n",
    "- Support vectors are data points are closer to the hyperplane and influence the position and orientation of the hyperplane.\n",
    "\n",
    "> **When to use SVM**:\n",
    "- Can be used for both regression and classification, but is widely used for classification.\n",
    "![image.png](attachment:image.png)\n",
    "[TDS Article](https://towardsdatascience.com/support-vector-machine-introduction-to-machine-learning-algorithms-934a444fca47)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are some benefits and drawbacks of the model?\n",
    "> **Benefits**:\n",
    "- Produces significant accuracy with less computational power.\n",
    "\n",
    "> **Drawbacks**:\n",
    "- Can only be used to predict a binary result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Give us an example of how to use it in Python using the provided dataset\n",
    "> **ANSWER BELOW**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.61\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "svm = SVC()\n",
    "svm.fit(X_train, y_train)\n",
    "y_pred = svm.predict(X_test)\n",
    "print(accuracy_score(y_test, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
