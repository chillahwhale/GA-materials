{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://imgur.com/1ZcRyrc.png\" style=\"float: left; margin: 20px; height: 55px\">\n",
    "\n",
    "## Regression Techniques\n",
    "\n",
    "_Your one-stop-shop for keeping regression techniques and code handy_\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will split up the work for this notebook:\n",
    "- Person 1: OLS (Linear Regression)\n",
    "- Person 2: Ridge Regression\n",
    "- Person 3: Lasso Regression\n",
    "- Person 4: Elastic Net Regression\n",
    "- Person 5: Least Angle Regression (LARS)\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regression models covered in this notebook:\n",
    "- [OLS (Linear Regression)](#Linear-Regression)\n",
    "- [Ridge Regression](#Ridge)\n",
    "- [Lasso Regression](#LASSO)\n",
    "- [Elastic Net Regression](#Elastic-Net)\n",
    "- [Least Angle Regression (LARS)](#LARS)\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/noellebrown/opt/anaconda3/lib/python3.7/site-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
      "  import pandas.util.testing as tm\n"
     ]
    }
   ],
   "source": [
    "# imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TV</th>\n",
       "      <th>radio</th>\n",
       "      <th>newspaper</th>\n",
       "      <th>sales</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>230.1</td>\n",
       "      <td>37.8</td>\n",
       "      <td>69.2</td>\n",
       "      <td>22.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>44.5</td>\n",
       "      <td>39.3</td>\n",
       "      <td>45.1</td>\n",
       "      <td>10.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>17.2</td>\n",
       "      <td>45.9</td>\n",
       "      <td>69.3</td>\n",
       "      <td>9.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>151.5</td>\n",
       "      <td>41.3</td>\n",
       "      <td>58.5</td>\n",
       "      <td>18.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>180.8</td>\n",
       "      <td>10.8</td>\n",
       "      <td>58.4</td>\n",
       "      <td>12.9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      TV  radio  newspaper  sales\n",
       "0  230.1   37.8       69.2   22.1\n",
       "1   44.5   39.3       45.1   10.4\n",
       "2   17.2   45.9       69.3    9.3\n",
       "3  151.5   41.3       58.5   18.5\n",
       "4  180.8   10.8       58.4   12.9"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import data from: http://faculty.marshall.usc.edu/gareth-james/ISL/data.html\n",
    "df = pd.read_csv('http://faculty.marshall.usc.edu/gareth-james/ISL/Advertising.csv')\n",
    "df.drop(columns='Unnamed: 0', inplace=True) # drop extra index column\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into testing and training sets\n",
    "features = ['TV', 'radio', 'newspaper']\n",
    "X = df[features] # feature matrix\n",
    "y = df['sales'] # target vector\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Linear Regression\n",
    "_Noelle_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When should you use the algorithm?\n",
    "> **When to use**:\n",
    "- When you are modeling linear data\n",
    "- When you need a regression model that is extremely interpretable and simple\n",
    "- When you need to see the influence of specific features\n",
    "- When your model is not overfit (if so, try regularization - see next sections)  \n",
    "Read more [here](https://statisticsbyjim.com/regression/choose-linear-nonlinear-regression/)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are some benefits of the model?\n",
    "> **Benefits**:\n",
    "- Computationally cheap, simple, fast, & easy\n",
    "- Very interpretable\n",
    "- Can get lots of statistics to help you understand the impact of features\n",
    "\n",
    "> **Drawbacks**:  \n",
    "- May not perform as well as other models\n",
    "- Data must be linear (meet LINE assumptions)\n",
    "- Cannot model very complicated relationships/curves in data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Give us an example of how to use it in Python using the provided dataset\n",
    "> **ANSWER BELOW**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "\n",
    "lin_reg = LinearRegression() # instantiate\n",
    "lin_reg.fit(X_train, y_train) # fit\n",
    "\n",
    "# simple evaluation function for scoring\n",
    "def evaluate(y_train_true, y_train_pred, y_test_true, y_test_pred):\n",
    "    print('Evaluation Metrics')\n",
    "    print('---------------------------')\n",
    "    print('Training r2:', r2_score(y_train_true, y_train_pred))\n",
    "    print('Testing r2:', r2_score(y_test_true, y_test_pred))\n",
    "    print(' ')\n",
    "    print('Training MAE:', mean_absolute_error(y_train_true, y_train_pred))\n",
    "    print('Testing MAE:', mean_absolute_error(y_test_true, y_test_pred))\n",
    "    print(' ')\n",
    "    print('Training MSE:', mean_squared_error(y_train_true, y_train_pred))\n",
    "    print('Testing MSE:', mean_squared_error(y_test_true, y_test_pred))\n",
    "    print(' ')\n",
    "    print('Training RMSE:', np.sqrt(mean_squared_error(y_train_true, y_train_pred)))\n",
    "    print('Testing RMSE:', np.sqrt(mean_squared_error(y_test_true, y_test_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Metrics\n",
      "---------------------------\n",
      "Training r2: 0.8966445527601498\n",
      "Testing r2: 0.8935163320163657\n",
      " \n",
      "Training MAE: 1.2156188847355558\n",
      "Testing MAE: 1.4023124989385078\n",
      " \n",
      "Training MSE: 2.7678910780469734\n",
      "Testing MSE: 2.880023730094194\n",
      " \n",
      "Training RMSE: 1.6636980128758263\n",
      "Testing RMSE: 1.6970632663793634\n"
     ]
    }
   ],
   "source": [
    "evaluate(y_train, lin_reg.predict(X_train), y_test, lin_reg.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Metrics\n",
      "---------------------------\n",
      "Training r2: 0.9861997890896803\n",
      "Testing r2: 0.9855413464588894\n",
      " \n",
      "Training MAE: 0.4128744495535545\n",
      "Testing MAE: 0.5137614721126593\n",
      " \n",
      "Training MSE: 0.3695739477107388\n",
      "Testing MSE: 0.3910577658726892\n",
      " \n",
      "Training RMSE: 0.6079259393303915\n",
      "Testing RMSE: 0.6253461168606463\n"
     ]
    }
   ],
   "source": [
    "# using pipelines & Polynomial Regression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# instantiate pipeline using Polynomial Features & Linear regression\n",
    "pipe = Pipeline([\n",
    "    ('poly', PolynomialFeatures()), # default degree is 2, I will leave this\n",
    "    ('lin_reg', LinearRegression())\n",
    "])\n",
    "\n",
    "# fit pipeline\n",
    "pipe.fit(X_train, y_train)\n",
    "\n",
    "# score pipeline\n",
    "evaluate(y_train, pipe.predict(X_train), y_test, pipe.predict(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Ridge\n",
    "_Matt_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When should you use the algorithm?  \n",
    "> What is Ridge Regression?\n",
    "- Ridge regression, like Lasso regression, is a regularization technique. That is, a technique used to deal with overfitting and when a dataset is large. More specifically, it is a technique that introduces increased bias in exchange for decreased variance. \n",
    "- Ridge regression uses a penalty parameter (alpha) to shrink our coefficients toward zero to simplify our model. The larger alpha, the greater the penalty. Note that, unlike in Lasso regression, in Ridge regression our coefficients shrink toward zero but never zero out. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are some benefits of the model?\n",
    "> What are some benefits of the model? When should you use the algorithm?\n",
    "- **Helps us to avoid overfitting.** Another way to say this is that Ridge regression reduces variance by introducing bias into our estimates. We do this by adding a penalty term, alpha, as described above. \n",
    "- **When we have large datasets.** Especially when we have \"short and fat\" data -- lots of features relative to the number of rows. This is particularly relevant when we have computational constraints.\n",
    "- **When we have highly correlated features.** Ridge regression will include all of the coefficients in the model but the they will be distributed depending on the correlation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Give us an example of how to use it in Python using the provided dataset\n",
    "> **ANSWER BELOW**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train r2: 0.8932824104762538\n",
      "test r2: 0.8905897730203325\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "# scale our data\n",
    "ss = StandardScaler()\n",
    "Z_train = ss.fit_transform(X_train)\n",
    "Z_test = ss.transform(X_test)\n",
    "# instantiate\n",
    "ridge = Ridge(alpha=10)\n",
    "# fit\n",
    "ridge.fit(Z_train, y_train)\n",
    "print(f'train r2: {ridge.score(Z_train, y_train)}')\n",
    "print(f'test r2: {ridge.score(Z_test, y_test)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> sources:\n",
    "- https://www.quora.com/What-are-the-benefits-of-using-ridge-regression-over-ordinary-linear-regression\n",
    "- https://www.analyticsvidhya.com/blog/2016/01/ridge-lasso-regression-python-complete-tutorial/#three\n",
    "- https://www.youtube.com/watch?v=OEU22e20tWw\n",
    "- lesson 4.05-regulatization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# LASSO\n",
    "_Nate_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When should you use the algorithm?\n",
    "> **ANSWER HERE**:  \n",
    "> [LASSO](https://www.statisticshowto.com/lasso-regression/) regression should be used when a model has high [multicollinearity](https://www.statisticshowto.com/multicollinearity/) or when you have a lot of variables and want to automate feature selection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are some benefits of the model?\n",
    "> **ANSWER HERE**:  \n",
    "> It's great for features selection, helps avoid overfitting, and retains interpretability.  \n",
    "> [More pros and cons of LASSO regression](https://www.quora.com/What-are-the-pros-and-cons-of-lasso-regression)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Give us an example of how to use it in Python using the provided dataset\n",
    "> **ANSWER BELOW**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso, LassoCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "# Set up a list of Lasso alphas to check.\n",
    "l_alphas = np.logspace(-3, 1, 100)\n",
    "# Cross-validate over our list of Lasso alphas.\n",
    "lasso_cv = LassoCV(\n",
    "    alphas=l_alphas,\n",
    "    cv=5,\n",
    "    max_iter=5000)\n",
    "# Fit model using best ridge alpha!\n",
    "lasso_cv.fit(Z_train, y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alpha: 0.07924828983539177\n",
      "Train: 0.896105190390421\n",
      "Test: 0.8950875719765927\n",
      "TV: 3.7774284647455936\n",
      "Radio: 2.7886904205062333\n",
      "Newspaper: 0.0\n"
     ]
    }
   ],
   "source": [
    "# Here is the optimal value of alpha\n",
    "print('alpha:', lasso_cv.alpha_)\n",
    "# R2 Scores\n",
    "print('Train:', lasso_cv.score(Z_train, y_train))\n",
    "print('Test:', lasso_cv.score(Z_test, y_test))\n",
    "# Coefficients\n",
    "print('TV:', lasso_cv.coef_[0])\n",
    "print('Radio:', lasso_cv.coef_[1])\n",
    "print('Newspaper:', lasso_cv.coef_[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Elastic Net\n",
    "_Colin_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is it?\n",
    "Elastic Net adds both penalties from Ridge and LASSO models:\n",
    "$$\n",
    "\\begin{eqnarray}\n",
    "SSE + Ridge + Lasso &=& \\sum_{i=1}^n \\left(y_i - \\hat{y}_i\\right)^2 + \\alpha\\left[\\rho\\sum_{j=1}^p |\\beta_j| + (1-\\rho)\\sum_{j=1}^p \\beta_j^2\\right] \\\\\n",
    "&=& \\|\\mathbf{y} - \\mathbf{X}\\beta\\|^2 + \\alpha\\left(\\rho\\|\\beta\\|_1 + (1 - \\rho)\\|\\beta\\|^2\\right)\n",
    "\\end{eqnarray}\n",
    "$$\n",
    "- The rho(looks like p) variable is a hyperparameter that tunes how much of each model you want to use. \n",
    "    - rho = 0.5 is 50% Ridge and 50% LASSO\n",
    "    - rho = 0.7 is 70% Ridge and 30% LASSO\n",
    "- Alpha dials the whole penalty up or down, so functions the same as alpha Ridge or LASSO."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why and when would we use it?\n",
    "<br>\n",
    "<details>\n",
    "        - \"My preference for elastic net is rooted in my skepticism that one will confidently know that ùêø1 or ùêø2    is the true model.\" - some guy on stats.stackexchange.com\n",
    "</details>\n",
    "<br>   \n",
    "<details>\n",
    "        - \"It's cute but you don't need it\" - Tim Book\n",
    "</details>\n",
    "<br>\n",
    "<details>\n",
    "<br>   \n",
    "        - \"Elastic net is always preferred over lasso & ridge regression because it solves the limitations of both methods, while also including each as special cases. So if the ridge or lasso solution is, indeed, the best, then any good model selection routine will identify that as part of the modeling process.\" - same guy on stats.stackexchange.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Give us an example of how to use it in Python using the provided dataset\n",
    "> **ANSWER BELOW**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import ElasticNet, ElasticNetCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "#preprocessing\n",
    "ss = StandardScaler()\n",
    "Z_train = ss.fit_transform(X_train)\n",
    "Z_test = ss.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8966161917256624\n",
      "0.8936580334433731\n"
     ]
    }
   ],
   "source": [
    "# Hyperparamaters\n",
    "enet_alphas = np.logspace(-3, 2, 100)\n",
    "rho = 0.5\n",
    "# Instance\n",
    "enet = ElasticNetCV(alphas=enet_alphas, l1_ratio=rho, cv=5)\n",
    "# Fit\n",
    "enet.fit(Z_train, y_train)\n",
    "# rho = 0.5\n",
    "print(enet.score(Z_train, y_train))\n",
    "print(enet.score(Z_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# LARS\n",
    "_Justin_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://medium.com/acing-ai/what-is-least-angle-regression-lar-bb86756f01d0\n",
    "https://blog.echen.me/2011/04/21/a-mathematical-introduction-to-least-angle-regression/\n",
    "http://statweb.stanford.edu/~tibs/ftp/lars.pdf\n",
    "https://en.wikipedia.org/wiki/Least-angle_regression\n",
    "https://stats.stackexchange.com/questions/404984/writing-by-hand-first-steps-in-least-angle-regression-lars\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lars.html\n",
    "https://www.quora.com/What-is-Least-Angle-Regression-and-when-should-it-be-used\n",
    "\n",
    "When should you use the algorithm?\n",
    "> **ANSWER HERE**: When it is a linear problem, but there are many variables with different impacts and correlations.  ALL PREDICTORS MUST BE STANDARDIZED FIRST (this is a default arguement when instantiating). It will start with the one that is most correlated with the residuals (which is $r = y - \\bar{y}$).  It will then increase the coefficient until another variable is showing similar correlation with the residuals.  Then that variable is added and coefficients are fitted again until another variable can be added.  This process continues until the model is big enough/used all the features.  The model is optimizing for ordinary least squares.  This will also \"turn off\" variables that are overly correlated with others and thusly have no impact.\n",
    ">It is a form of forward stepwise regression\n",
    ">This will also help crackdown on overfitting\n",
    "![](https://miro.medium.com/max/1400/1*_uwNycdc2EjJlGBWZKUFZQ.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are some benefits of the model?\n",
    "> **ANSWER HERE**: Easy to explain.  Efficient to run.  Starts as a simple model and increases complexity as it iterates over the variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Give us an example of how to use it in Python using the provided dataset\n",
    "> **ANSWER BELOW**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'TV': 0.04543, 'radio': 0.19146, 'newspaper': 0.00257}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import Lars\n",
    "larsreg = Lars()\n",
    "larsreg.fit(X_train, y_train)\n",
    "lars_coefs = dict(zip(df.columns.tolist()[:-1], \n",
    "                      np.round(larsreg.coef_, 5)))\n",
    "lars_coefs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LARS training: 0.8966\n",
      "LARS testing: 0.8935\n"
     ]
    }
   ],
   "source": [
    "print(f'LARS training: {larsreg.score(X_train, y_train):.4}')\n",
    "print(f'LARS testing: {larsreg.score(X_test, y_test):.4}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# zip code for coefs dictionary adopted from:\n",
    "# https://towardsdatascience.com/a-comparison-of-shrinkage-and-selection-methods-for-linear-regression-ee4dd3a71f16"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
