{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://imgur.com/1ZcRyrc.png\" style=\"float: left; margin: 15px; height: 80px\">\n",
    "\n",
    "# Quiz 4 Review\n",
    "\n",
    " _**Author:** Boom D. (DSI-NYC), edited by Noelle B. (DSI-DEN)_\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tree-Based Classification Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q1.** Briefly explain what a decision tree is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> A decision tree is a non-parametric supervised ML model that can be used for regression or classification problems. It works by creating hierarchical splits based on the measure of purity (how separated the classes are) to determine the prediction of the observation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q2.** Briefly describe what the Gini impurity is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Gini impurity is a measure of how well separated the nodes are (you want your splits to be as separated as possible). More formally, it is the \"measurement of the likelihood of an incorrect classification of a new instance of a random variable, if that new instance were randomly classified according to the distribution of class labels from the data set\" [(source)](https://bambielli.com/til/2017-10-29-gini-impurity/).\n",
    "\n",
    "$$\n",
    "\\begin{eqnarray*}\n",
    "\\text{Gini impurity} &=& 1 - \\sum_{i=1}^{classes} P(\\text{class i})^2 \\\\\n",
    "\\text{Gini impurity (2 classes)} &=& 1 - P(\\text{class 1})^2 - P(\\text{class 2})^2 \\\\\n",
    "\\text{Gini impurity (3 classes)} &=& 1 - P(\\text{class 1})^2 - P(\\text{class 2})^2 - P(\\text{class 3})^2 \\\\\n",
    "\\end{eqnarray*}\n",
    "$$\n",
    "\n",
    "> - A Gini score of 0 means all of our observations are from the same class!\n",
    "> - In the binary case, Gini impurity ranges from 0 to 0.5.\n",
    "> - If we have three classes, Gini impurity ranges from 0 to 0.66667.\n",
    "> - If we have $k$ classes, Gini impurity ranges from 0 to $1-\\frac{1}{k}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q3.** What would the Gini impurity be if all observations fell into the same class?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Gini impurity would be zero.\n",
    "\n",
    "$$\n",
    "\\begin{eqnarray*}\n",
    "\\text{Gini impurity} &=& 1 - \\sum_{i=1}^{classes} P(\\text{class i})^2 \\\\\n",
    "&=& 1 - P(\\text{class 1})^2 \\\\\n",
    "&=& 1 - 1^2 \\\\\n",
    "&=& 1 - 1 \\\\\n",
    "&=& 0\n",
    "\\end{eqnarray*}\n",
    "$$\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q4.** TRUE/FALSE: The Gini impurity score of the following set is 0.25.  \n",
    "pets = ['dog', 'cat', 'dog', 'cat']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> FALSE: Gini impurity for two classes each with two items is 0.5:\n",
    "\n",
    "$$\n",
    "\\begin{eqnarray*}\n",
    "\\text{Gini impurity} &=& 1 - \\sum_{i=1}^{classes} P(\\text{class i})^2 \\\\\n",
    "&=& 1 - P(\\text{class dog})^2 - P(\\text{class cat})^2 \\\\\n",
    "&=& 1 - \\left(\\frac{1}{2}\\right)^2 - \\left(\\frac{1}{2}\\right)^2 \\\\\n",
    "&=& 1 - \\frac{1}{4} - \\frac{1}{4} \\\\\n",
    "&=& \\frac{1}{2}\n",
    "\\end{eqnarray*}\n",
    "$$\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q5.** Explain what we mean by _bootstrapping_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Sampling with replacement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q6.** Briefly describe the role of bootstrapping in algorithms like Bagging, Random Forests, and Boosting (i.e. what issue does it try to address?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Ensemble algorithms fit on random samples with replacement to counteract overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q7.** What is the advantage of ensembling methods vs. non-ensembling methods?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Wisdom of the crowds, ensembling methods tend to be more accurate than non-ensembling methods (think about the Penelope the cow in-class activity)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q8.** Using `numpy`, write a single line of code to generate a bootstrapped sample of size 100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 3, 5, 2, 2, 1, 7, 2, 5, 2, 3, 5, 5, 7, 8, 2, 4, 5, 8, 4, 2, 1,\n",
       "       1, 8, 8, 1, 1, 7, 2, 3, 8, 7, 2, 7, 2, 2, 8, 2, 4, 3, 8, 4, 1, 8,\n",
       "       1, 2, 4, 3, 2, 5, 7, 1, 3, 4, 5, 3, 4, 8, 8, 1, 3, 8, 4, 1, 4, 1,\n",
       "       3, 5, 2, 8, 5, 2, 4, 2, 5, 1, 8, 8, 1, 8, 2, 8, 5, 1, 5, 5, 7, 1,\n",
       "       5, 7, 7, 1, 2, 1, 2, 1, 8, 2, 8, 5])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "list_data = [1, 4, 7, 3, 2, 8, 2, 5, 8, 1]\n",
    "np.random.seed(42)\n",
    "\n",
    "# Your code below\n",
    "np.random.choice(list_data, 100, replace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q9.** Briefly explain how the **Random Forest Classifier** works."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Random Forest classifiers are an ensemble method that are comprised of many decision trees. Each tree is fit on a boostrapped sample and each node is given a subset of features to determine the best split. They tend to be more accurate than a single decision tree and are much less prone to overfitting due to the voting of the many trees."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q10.** If I build a CART classification tree for a set of data and see that my Train Accuracy = 0.99 but Test Accuracy = 0.75, what kind(s) of \"enhanced\" decision tree classifiers can I apply to improve this situation?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The high training score but low testing score indicates overfiting, you could use a Random Forest to lower variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q11.** Now suppose I built a CART classification tree where Train Accuracy = 0.182 but Test Accuracy = 0.181, what kind(s) of \"enhanced\" decision tree classifiers can I apply to improve this situation?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The low training and testing scores indicate underfiting (bad model), you could maybe try a boosting classifier (since the model seems to be weak)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Machines (SVM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q12.** Briefly describe Support Vector Machines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> SVMs try to find the `line of separability` that best separates the classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q13.** Describe the purpose of the kernel in the SVM algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The kernel trick adds dimensionality to find that line of separability in higher-order planes. This allows for SVMs to work when data is not linearly-seperable in a lower dimension."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generalized Linear Models (GLM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q14.** What kind of GLM would be most appropriate if I wanted to build a model to predict the number of customers who will walk into the Liquiteria store on 51st St and Lexington Ave in a given hour given a bunch of features on customer preferences and behavior?\n",
    "\n",
    "(A) Logistic Regression <br>\n",
    "(B) Poisson Regression <br>\n",
    "(C) Gamma Regression <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Poisson (successes in a given time period)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q15.** What kind of GLM would be most appropriate if I wanted to build a model to predict the time until Trump posts another controversial tweet given a bunch of features?\n",
    "\n",
    "(A) Logistic Regression <br>\n",
    "(B) Poisson Regression <br>\n",
    "(C) Gamma Regression <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Gamma (time until next success)\n",
    "\n",
    "> \"Gamma distribution is a distribution that arises naturally in processes for which the waiting times between events are relevant. It can be thought of as a waiting time between Poisson distributed events.\" [Source & read more about the Gamma distribution here](http://wiki.stat.ucla.edu/socr/index.php/AP_Statistics_Curriculum_2007_Gamma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q16.** What kind of GLM would be most appropriate if I wanted to build a model to predict the probability that a customer unsubscribes from a magazine given a bunch of features on customer characteristics?\n",
    "\n",
    "(A) Logistic Regression <br>\n",
    "(B) Poisson Regression <br>\n",
    "(C) Gamma Regression <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Logistic Regression (predicted probability of success)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q17.** Gradient descent is given by the following formula :  \n",
    "\n",
    "$\\hat{\\beta}_{1, i+1} := \\hat{\\beta}_{1, i} - \\alpha[\\frac{\\partial L}{\\partial \\beta_1}]$\n",
    "\n",
    "In the formula, what does $\\alpha$ represent?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> alpha controls the \"learning rate\" or how big each step to take in the direction of highest negative slope (step_size)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q18.** I fit a model. When “fitting” the model, the computer uses gradient descent to\n",
    "estimate the parameters of the model. After fitting the model, Python returns a `ConvergenceWarning`. Discuss possible reasons why this happened and how we can remedy this situation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> A Convergence Warning means that the algorithm never converged, it never got to a final answer. To remedy this, you can modify the learning rate (alpha), but you shouldn't increase max_iter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SQL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For questions #19 - 21, use the following `bball` table:  \n",
    "\n",
    "|name|position|points|height|\n",
    "|---|---|---|---|\n",
    "|Trent|Point Guard|12|6'3\"|\n",
    "|Allan|Forward|23|6'|\n",
    "|Bryan|Center|9|6'8\"|\n",
    "|Bill|Point Guard|20|6'7\"|\n",
    "\n",
    "**Q19.** Write a SQL query to return the name and height of players that scored at least 20 points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> SELECT b.name, b.height  \n",
    "> FROM bball AS b \n",
    "> WHERE points >= 20;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q20.** Write a SQL query to return all of the player names that begin with `B`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> SELECT name \n",
    "> FROM bball  \n",
    "> WHERE name LIKE 'B%';"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q21.** Write a SQL query to return the average points by position type."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> SELECT position, AVG(points)  \n",
    "> FROM bball  \n",
    "> GROUP BY position;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
