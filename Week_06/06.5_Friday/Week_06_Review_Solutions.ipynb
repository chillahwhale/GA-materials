{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://imgur.com/1ZcRyrc.png\" style=\"float: left; margin: 15px; height: 80px\">\n",
    "\n",
    "# Week 6 Review - Solutions\n",
    "\n",
    " _**Author:** Noelle B. (DSI-DEN)_\n",
    "\n",
    "---\n",
    "We will review the learning objectives of each lesson this week and answer questions related to them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5.07 Object Oriented Programming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explain why object-oriented programming (OOP) is central to the Python programming language."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q1.** Define `object oriented programming` and `functional programming` and give a few examples of each paradigm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Answer:**  \n",
    "- **Object oriented programming**: Creating your own data types that can maintain their own state and have methods & functions associated with them. Ex: Python, Ruby\n",
    "- **Functional Programming**: Completing all tasks using functions in clever ways. Ex: R, Scala"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build a Python class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q2.** Build a `Tree` class in python that takes in at least a `tree_type` attribute and has at least one method (ex. grow)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer:\n",
    "class Tree:\n",
    "    def __init__(self, tree_type, height):\n",
    "        self.tree_type = tree_type\n",
    "        self.height = height\n",
    "    \n",
    "    def grow(self, grow_amount):\n",
    "        print('Growing...')\n",
    "        self.height += grow_amount\n",
    "        print(f'Done! My new height is {self.height}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Growing...\n",
      "Done! My new height is 87\n"
     ]
    }
   ],
   "source": [
    "new_tree = Tree('aspen', 72)\n",
    "new_tree.grow(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Describe object mutability in Python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q3.** What is object mutability in Python?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Answer:**  \n",
    "The state of an object can change. For example, we changed the height of the tree in the above example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understand how Scikit-Learn uses OOP to build estimators and transformers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q4.** Beifly describe how Scikit-Learn uses OOP to build estimators and transformers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Answer:**  \n",
    "Sklearn uses OOP to build estimators and transformers. This can be seen in the code for sklearn and is the reason that there are certain attributes that can only be accessed after an estimator is fit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up their working directory to import a class from another file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q5.** If I had a `Tree` class in my `growtree.py` file, how would I import this into Python?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer:\n",
    "from growtree import Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6.01 Classification and Regression Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understand the intuition behind decision trees."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q6.** Briefly describe what a decision tree is and how it works."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Answer:**  \n",
    "A decision tree is a model that attempts to predict values based on a series of splits. It is comprised of a root node, parent nodes, child nodes, and finally leaf nodes that depict the prediction of the value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate Gini."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q7.** Calculate the Gini impurity for the following list:  \n",
    "`cities = ['Denver', 'Fort Collins', 'Boulder', 'Fort Collins', 'Denver']`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Answer:**  \n",
    "Gini impurity = 1 - sum(P(class i)^2  \n",
    "= 1 - P('Denver')^2 - P('Fort Collins')^2 - P('Boulder')^2  \n",
    "= 1 - (2/5)^2 - (2/5)^2 - (1/5)^2  \n",
    "= 1 - (4/25) - (4/25) - (1/25)  \n",
    "= 16/25  \n",
    "= 0.64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Describe how decision trees use gini to make decisions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q8.** How is gini used in decision trees to make decisions?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Answer:**  \n",
    "The nodes are determined by finding the feature that would decrease the gini impurity by the most from the parent node to the child node."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit, generate predictions from, and evaluate decision tree models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q9.** Using the following dataset, fit, generate predictions from, and evaluate a decision tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# load data\n",
    "bc = load_breast_cancer()\n",
    "bcdf = pd.DataFrame(bc.data, columns = bc.feature_names)\n",
    "bcdf['type'] = bc.target\n",
    "\n",
    "X = bcdf.drop(columns='type')\n",
    "y = bcdf['type']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 1.0\n",
      "Testing accuracy: 0.951048951048951\n"
     ]
    }
   ],
   "source": [
    "# Answer:\n",
    "dt = DecisionTreeClassifier()\n",
    "\n",
    "dt.fit(X_train, y_train)\n",
    "print('Training accuracy:', dt.score(X_train, y_train))\n",
    "print('Testing accuracy:', dt.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Interpret and tune max_depth, min_samples_split, min_samples_leaf, ccp_alpha & visualize a decision tree."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q10.** Describe what the following hyperparameters do in decision trees:\n",
    "- max_depth\n",
    "- min_samples_split\n",
    "- min_samples_leaf\n",
    "- ccp_alpha"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Answer:** (all from global lesson 6.01)  \n",
    "- `max_depth`: The maximum depth of the tree. By default, the nodes are expanded until all leaves are pure (or some other argument limits the growth of the tree). In the 20 questions analogy, this is like \"How many questions we can ask?\"\n",
    "- `min_samples_split`: The minimum number of samples required to split an internal node. By default, the minimum number of samples required to split is 2. That is, if there are two or more observations in a node and if we haven't already achieved maximum purity, we can split it!\n",
    "- `min_samples_leaf`: The minimum number of samples required to be in a leaf node (a terminal node at the end of the tree). By default, the minimum number of samples required in a leaf node is 1. (This should ring alarm bells - it's very possible that we'll overfit our model to the data!)\n",
    "- `ccp_alpha`: A complexity parameter similar to $\\alpha$ in regularization. As ccp_alpha increases, we regularize more. By default, this value is 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6.02 Bagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define ensemble model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q11.** Define `ensemble model`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Answer:**   \n",
    "Ensemble models are several weak learners that combine to form a strong learner. Wisdom of the crowds."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Name three advantages of using ensemble models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q12.** Name three advantages of using ensemble models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Answer:** (from global lesson 6.02)  \n",
    "- The `statistical` benefit to ensemble methods: By building one model, our predictions are almost certainly going to be wrong. Predictions from one model might overestimate housing prices; predictions from another model might underestimate housing prices. By \"averaging\" predictions from multiple models, we'll see that we can often cancel our errors out and get closer to the true function $f$.\n",
    "- The `computational` benefit to ensemble methods: It might be impossible to develop one model that globally optimizes our objective function. (Remember that CART reach locally-optimal solutions that aren't guaranteed to be the globally-optimal solution.) In these cases, it may be impossible for one CART to arrive at the true function $f$. However, generating many different models and averaging their predictions may allow us to get results that are closer to the global optimum than any individual model.\n",
    "- The `representational` benefit to ensemble methods: Even if we had all the data and all the computer power in the world, it might be impossible for one model to exactly equal $f$. For example, a linear regression model can never model a relationship where a one-unit change in $X$ is associated with some different change in $Y$ based on the value of $X$. All models have some shortcomings. (See the no free lunch theorems.) While individual models have shortcomings, by creating multiple models and aggregating their predictions, we can actually create predictions that represent something that one model cannot ever represent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define and execute bootstrapping."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q13.** Define `bootstrapping`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Answer:**  \n",
    "Bootstrapping is the process of generating a sample of the data with replacement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q14.** Create a bootstrapped sample given the following data:  \n",
    "`example = [1, 4, 5, 2, 12, 6, 3, 19, 27, 10, 24, 42]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "example = [1, 4, 5, 2, 12, 6, 3, 19, 27, 10, 24, 42]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 6,  2,  5,  3, 24,  1, 19,  1, 42,  3,  6,  6])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Answer:\n",
    "np.random.choice(example, size=len(example), replace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit and evaluate bagged decision trees."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q15.** Using the following dataset, fit and evaluate bagged decision trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "\n",
    "# load data\n",
    "bc = load_breast_cancer()\n",
    "bcdf = pd.DataFrame(bc.data, columns = bc.feature_names)\n",
    "bcdf['type'] = bc.target\n",
    "\n",
    "X = bcdf.drop(columns='type')\n",
    "y = bcdf['type']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 0.9929577464788732\n",
      "Testing accuracy: 0.9440559440559441\n"
     ]
    }
   ],
   "source": [
    "# Answer\n",
    "bag = BaggingClassifier(random_state=42) # default is decision tree\n",
    "\n",
    "bag.fit(X_train, y_train)\n",
    "print('Training accuracy:', bag.score(X_train, y_train))\n",
    "print('Testing accuracy:', bag.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6.03 Random Forests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define ensemble model and describe three rationales for using ensemble models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q16.** What is an ensemble model and why would you use them?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Answer:**   \n",
    "Ensemble models are many weak learners combined to make a strong learner. They tend to be less overfit and more accurate than single models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execute bootstrapping."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q17.** How is bootstrapping used in Random Forest models?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Answer:**  \n",
    "Each random forest tree is built using a bootstrapped sample of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Describe the differences among and implement bagged decision trees, random forests, and ExtraTrees."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q18.** Describe the differences between bagged decision trees, random forests, and ExtraTrees."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Answer:**  \n",
    "In all three models, bootstrapped samples are used to build each decision tree and the results are aggregated to come up with a final prediction. Random forests are like bagged decision trees, but each node only sees a random subset of features. ExtraTrees are like random forests, but the split is randomly chosen for each feature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identify advantages and disadvantages of using CART models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q19.** What are some advantages and disadvantages of using CART models?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Answer:**  \n",
    "- Advantages: can have high accuracy, are simple to explain, interpretable\n",
    "- Disadvantages: tendency to overfit (hence why we use things like random forests)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6.04 Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understand the differences between bagging and boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q20.** What is the difference between bagging and boosting?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Answer:**  \n",
    "Bagging uses many models trained individually and aggregates the results. Boosting uses the results of the previous model to create a hopefully better model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understand how boosting is an ensemble method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q21.** Describe how boosting works."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Answer:**  \n",
    "In general, boosting combines many weak learners one at a time to create a strong learner, iterating off the previous results to make the model better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learn the pros and cons to using boosting models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q22.** What are the pros and cons to using a gradient-boosting model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Answer:** (from global lesson 6.04)  \n",
    "- Pros: Natural handling of mixed data types (= heterogeneous features), Predictive power, Robustness to outliers in output space (via robust loss functions)\n",
    "- Cons: Scalability: Due to the sequential nature of boosting, it can hardly be parallelized, Difficult hyperparameters to tune"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learn the effect of boosting on the bias-variance trade-off"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q23.** What happens to bias and variance when using boosting models?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Answer:**  \n",
    "Bias tends to decrease and variance can sometimes decrease as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learn the math and procedure for AdaBoost, the \"classic\" boosting model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q24.** Briefly describe how AdaBoost works."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Answer:**  \n",
    "AdaBoost fits a sequence of weak lerners on repeatedly modified versions of the data. Each iteration it weights observations based on how well the model performed on the previous iteration. It focuses its next model on the misclassifications/weaknesses of the prior models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understand the differences between AdaBoost and gradient-boosting models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q25.** Briefly describe how AdaBoost is different from gradient-boosting models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Answer:**  \n",
    "AdaBoost adjusts each model based on the wrong classifications of the previous models, while gradient boosting uses the residuals of the previous model to improve the following model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6.05 Support Vector Machines (SVMs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Describe linear separability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q26.** What does it mean for data to be linearly separable?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Answer:**  \n",
    "Data that is linearly separable can be completely separated by a line."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Differentiate between maximal margin classifiers, support vector classifiers, and support vector machines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q27.** Briefly describe Maximal Margin SVMs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Answer:**  \n",
    "Maximal Margin SVMs attempt to find the best separating hyperplane that completely separates the categories."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q28.** Briefly describe Soft Margin SVMs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Answer:**  \n",
    "Soft Margin SVMs allow for a little more error than Maximal Margin SVMs. They find the best separating hyperplane that mostly separates the categories and measures the distance between wrongly classified points to the margin."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q29.** Briefly describe Kernel SVMs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Answer:**  \n",
    "Kernel SVMs take advantage of the Kernel Trick to increase dimensionality in order to better separate the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement SVMs in scikit-learn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q30.** Using the provided data, implement a Support Vector Classifier and score your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# data\n",
    "bc = load_breast_cancer()\n",
    "bcdf = pd.DataFrame(bc.data, columns = bc.feature_names)\n",
    "bcdf['type'] = bc.target\n",
    "X_train, X_test, y_train, y_test = train_test_split(bcdf.drop(columns='type'), bcdf['type'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 1.0\n",
      "Testing accuracy: 0.5944055944055944\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/noellebrown/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# Answer:\n",
    "svc = SVC()\n",
    "\n",
    "svc.fit(X_train, y_train)\n",
    "print('Training accuracy:', svc.score(X_train, y_train))\n",
    "print('Testing accuracy:', svc.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Describe the effects of C and kernels on SVMs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q31.** What is the Kernel trick and why is it useful in SVMs?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Answer:**  \n",
    "The Kernel trick increases the dimensionality of the data by curving the axis. This allows us to have high dimensionality without sacrificing computing ease."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6.06 Generalized Linear Models (GLMs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Describe generalized linear models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q32.** What are the three components of GLMs?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Answer:**  \n",
    "Systematic component, random component, and link function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit Poisson and Gamma regression models in statsmodels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q33.** Use the given data to fit a Poisson regression using statsmodels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "from scipy.stats import poisson\n",
    "\n",
    "# create random poisson data\n",
    "# https://www.datacamp.com/community/tutorials/probability-distributions-python\n",
    "# https://stats.stackexchange.com/questions/27443/generate-data-samples-from-poisson-regression\n",
    "X_data = np.random.choice(np.linspace(0, 15, 500), size=10000)\n",
    "y = poisson.rvs(mu=3, size=10000)\n",
    "X = sm.add_constant(X_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>Generalized Linear Model Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>           <td>y</td>        <th>  No. Observations:  </th>  <td> 10000</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                  <td>GLM</td>       <th>  Df Residuals:      </th>  <td>  9998</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model Family:</th>         <td>Poisson</td>     <th>  Df Model:          </th>  <td>     1</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Link Function:</th>          <td>log</td>       <th>  Scale:             </th> <td>  1.0000</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>                <td>IRLS</td>       <th>  Log-Likelihood:    </th> <td> -19237.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>            <td>Thu, 23 Apr 2020</td> <th>  Deviance:          </th> <td>  10774.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                <td>12:40:42</td>     <th>  Pearson chi2:      </th> <td>9.81e+03</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Iterations:</th>          <td>4</td>        <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>     <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "    <td></td>       <th>coef</th>     <th>std err</th>      <th>z</th>      <th>P>|z|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th> <td>    1.1014</td> <td>    0.012</td> <td>   95.335</td> <td> 0.000</td> <td>    1.079</td> <td>    1.124</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x1</th>    <td>   -0.0006</td> <td>    0.001</td> <td>   -0.427</td> <td> 0.669</td> <td>   -0.003</td> <td>    0.002</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                 Generalized Linear Model Regression Results                  \n",
       "==============================================================================\n",
       "Dep. Variable:                      y   No. Observations:                10000\n",
       "Model:                            GLM   Df Residuals:                     9998\n",
       "Model Family:                 Poisson   Df Model:                            1\n",
       "Link Function:                    log   Scale:                          1.0000\n",
       "Method:                          IRLS   Log-Likelihood:                -19237.\n",
       "Date:                Thu, 23 Apr 2020   Deviance:                       10774.\n",
       "Time:                        12:40:42   Pearson chi2:                 9.81e+03\n",
       "No. Iterations:                     4                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "const          1.1014      0.012     95.335      0.000       1.079       1.124\n",
       "x1            -0.0006      0.001     -0.427      0.669      -0.003       0.002\n",
       "==============================================================================\n",
       "\"\"\""
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Answer:\n",
    "# From global lesson 6.06\n",
    "glm_poi = sm.GLM(\n",
    "    y, X,\n",
    "    family=sm.families.Poisson(link = sm.families.links.log())\n",
    ").fit()\n",
    "\n",
    "glm_poi.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpret coefficients from Poisson and Gamma regression models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q34.** Suppose you get the following coefficient for your Poisson regression: `0.054`. Interpret the meaning of this coefficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.05548460215508"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.exp(0.054)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Answer:**  \n",
    "As $X_i$ increases by 1, I expect $Y$ to increase by a factor of $e^{\\beta_1}$.  \n",
    "So, as our X increases by 1, we expect y to increase by 1.055 times."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Describe iteratively reweighted least squares."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q35.** Briefly describe how iteratively reweighted least squares works."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Answer:** (from global lesson 6.06)  \n",
    "A solution is initially guessed, then iteratively refined until we converge on an answer. IRLS is a special cause of a gradient descent algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6.07 Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understand the intuition behind gradient descent & Implement gradient descent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q36.** Briefly describe how gradient descent works and what it is used for."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Answer:**  \n",
    "Gradient descent is used to find the values that minimize your loss function. The new position is calculated using the old position and subtracting alpha times the gradient/derivative of that position.\n",
    "1. Compute the derivative\n",
    "2. Move in the appropriate direction\n",
    "3. Repeat steps 1 & 2\n",
    "4. Stop when your move is small enough"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understand common pitfalls associated with gradient descent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q37.** What are a few common pitfalls in gradient descent?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Answer:**  \n",
    "Finding a local minimum instead of a global minimum, too large of an alpha, too small of an alpha."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identify solutions for common pitfalls."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q38.** What are some techniques to avoid failure to converge?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Answer:**  \n",
    "Change your alpha - if alpha is too small or too large it will not converge."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
